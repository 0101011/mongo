/*!
@page testing How WiredTiger is Tested

WiredTiger uses several different tools for testing WiredTiger - using
different tools allows us to effectively test different aspects of reliability.
The testing can be broken into the following categories:

- Functionality coverage

- Bug regression testing

- Cross platform testing

- Multi-threading testing

- Stress testing

- Performance testing

Details of the above test dimensions are discussed in more detail below.

Our tests are automatically run using the Jenkins (http://www.jenkins.org/)
continuous integration testing framework. This allows us to be confident
that we maintain support for all platforms, and aren't introducing performance
regressions over time.

@section test_suite Unit Test Suite

WiredTiger functional testing happens via our Python unit test suite. The
Python test suite can be found in the GitHub source tree under test/suite.

The WiredTiger Python test suite is built using a combinations of the
WiredTiger Python API and the Python unittest functionality. It requires at
least Python version 2.6.

The WiredTiger Python test suite implements a set of test cases that can be
run across the different platforms supported by WiredTiger. Each test case
is designed to test a specific functionality in a reproducible way - making
it simple to diagnose errors based on the test suite.

The WiredTiger test suite includes tests that cover:
- Functionality exposed via the WiredTiger API.

- Combinations of configuration settings for WiredTiger APIs. Including both
valid and invalid combinations.

- Bug regression tests that cover previous bugs, and avoid their reintroduction.

The WiredTiger Python test suite can be run in a short period of time
facilitated by running multiple test cases in parallel.

The test suite covers the following areas of functionality:

<b>Access Methods</b>

- row: (K/V pair)

- var: (like 64 bit recno) with arbitrary length value

- fix: (like 64 bit recno) but fixed (0-8 bit) value

<b> Datastore Operations</b>

- key based operations - create, update, delete
- cursor based operations - forw/back iteration, search, search near

<b>Secondary Indices</b>

create, update, delete from both primary and or secondary. multiple
secondaries, cursors, etc.

<b>Data Variations</b>

- key/value length

- key/value 'values': e.g. english, binary, sparse, unicode, etc.

- data types for 'typed' access (key_format, value_format)

The above areas are the 'basics' of the test suite.  We also 'mix in' other
dimensions:

<b>Disk Storage Variations</b>

- block_compressor off vs. bzip2

- {internal,leaf}_node_{min,max}

- column_{internal,leaf}_extend

- huffman encoding for key,value,both,none

- prefix_compression

- cache_size

- allocation_size variations

- btree (there are no other options currently)

- split_min, split_pct

- internal_key_truncate

- key_gap

Other than changing configuration options, the tests will be the same, and we
expect the results to be the same. These tests must be done completely - the
full combinatorial subset of these options against our basic tests.

Some notes:

- via dump or other stats, we may be able to detect that the desired
 configuration options were indeed used.

- Some combinations may in fact be illegal, some options may only apply to
 a particular access method, etc.

<b>Schema</b>

- column lists

- column groups (colgroups)

<b>Miscellany</b>

- opening with exclusive=true

- variations in file names (current, relative, abs directory)

- cursor types: bulk, dump, statistics, printable, raw

<b>Callbacks</b>

collator, compressor, ..., events

<b>Transaction and Isolation</b>

Cursor isolation levels: snapshot, read-committed, read-uncommitted.

<b>Command-line Utility</b>

- wt dump

- wt load

- wt salvage

- wt verify

<b>Capacity / Limits Testing.</b>

In any of the above functional tests, where appropriate, we should test:

- multiple (connections, sessions, cursors, etc.)

- capacity testing, e.g. testing 10000 simultaneously opened cursors, etc.

- wherever there are limits, test them.

- wherever there are no limits, run to some reasonable capacity.

- capacity and multiples in combinations

<b>Testing on Multiple Platforms.</b>

Once we have a test suite, it should be straightforward to try it on whatever
platform we can get WT running on.

<b>Regression Tests.</b>

Individual tests will be made that correspond to bug conditions we've
encountered. Whether they belong in the basic category or one of the mixins
will be determined on a case by case basis.

@section testing_threads Multithreading

Multithreaded testing is difficult. For the moment, there is a multithreading
test (in test/thread) written in C that is the primary engine for testing
this. There will be some simple Python tests for multithreading, though this
will not be a primary emphasis. One important issue is that we expect the
failures resulting from the python test base to be mostly repeatable. Adding
a large multithreading component with uncontrolled interactions is in
opposition to this goal. We will be periodically evaluating other ways to
repeatably test multithreaded behavior.

@section testing_stress Stress Testing

Similarly, stress testing is mostly handled by the test/format tool. Stress
tests can take significant time to run, which goes against a goal of having
a reasonable amount of time to run the entire suite (that is, less than
several days!)

@section testing_performance Performance Testing

We test performance using the wtperf tool, running a variety of configurations
on every push, and tracking any performance regressions.

 */
